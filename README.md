# OpenRouter ‚Äî Access 200+ AI Models with One API

Access OpenRouter models through MCP

> **One key. 200+ models. Semantic search finds the perfect model for any task.** Your AI can now call other AIs ‚Äî smarter ones, faster ones, cheaper ones, specialized ones.

[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey.svg)](https://github.com/AuraFriday/mcp-link-server)

---

## Benefits

### 1. üîç Semantic Model Search
**Not just a list ‚Äî intelligent discovery.** Search "best for code analysis" and get models ranked by actual capability match. Vector embeddings understand what you need, not just keywords.

### 2. üåê 200+ Models, One API
**Every major AI in one place.** GPT-4, Claude, Gemini, Llama, Mistral, and 200+ more. One API key, one interface, unlimited possibilities.

### 3. üí∞ Automatic Price Optimization
**Find the cheapest model that meets your needs.** Filter by price, context length, capabilities. Get enterprise performance at fraction of the cost.

---

## Why This Tool Changes AI Development

**Most developers stick to one model.** OpenAI's GPT-4, Anthropic's Claude, Google's Gemini ‚Äî pick one, hope it works for everything. But different tasks need different models.

**Model comparison is manual and tedious.** Visit provider websites, read specs, compare pricing, test each one. Hours of research for every new use case.

**Switching providers means rewriting code.** Different APIs, different formats, different authentication. Lock-in by inconvenience.

**This tool solves all of that.**

Semantic search: "Find models good at math" ‚Üí Instant ranked list.  
One API: Same code works for GPT-4, Claude, Llama, Mistral, everything.  
Price filtering: "Under $0.0001 per token, 100K+ context" ‚Üí Perfect matches.  
Auto-refresh: Model database updates daily. Always current.

---

## Real-World Story: The Cost Optimization Discovery

**The Problem:**

A startup was using GPT-4 for everything:
- Customer support responses ($0.03/1K tokens)
- Code analysis ($0.03/1K tokens)
- Simple classifications ($0.03/1K tokens)
- Data extraction ($0.03/1K tokens)

Monthly AI bill: **$12,000**

"Can we reduce costs without sacrificing quality?" they asked.

Standard approach: Manually test cheaper models, hope they work, deal with API differences. **Estimated time: 2-3 weeks of engineering.**

**With This Tool:**

```python
# Find models for simple classification (cheap, fast)
simple_models = search_models(
    bindings={"query_vec": {"_embedding_text": "fast classification and categorization"}},
    sql="""SELECT id, context_length, prompt_cost, completion_cost, 
           vec_distance_cosine(embedding, vec_f32(:query_vec)) as similarity 
           FROM models 
           WHERE prompt_cost < 0.0001 
           ORDER BY similarity LIMIT 5"""
)

# Find models for code analysis (quality matters)
code_models = search_models(
    bindings={"query_vec": {"_embedding_text": "code analysis and reasoning"}},
    sql="""SELECT id, context_length, prompt_cost, completion_cost,
           vec_distance_cosine(embedding, vec_f32(:query_vec)) as similarity 
           FROM models 
           WHERE context_length > 32000 
           ORDER BY similarity LIMIT 5"""
)

# Test each model with sample data
for model in simple_models:
    result = chat_completion(
        model=model['id'],
        messages=[{"role": "user", "content": "Classify this support ticket..."}]
    )
    # Evaluate quality vs cost
```

**Result:**

- Customer support: Switched to Llama 3.1 70B ($0.0004/1K tokens) ‚Äî 98% accuracy, **98.7% cost reduction**
- Code analysis: Kept GPT-4 (quality critical)
- Simple classification: Switched to Mistral 7B ($0.0001/1K tokens) ‚Äî 95% accuracy, **99.7% cost reduction**
- Data extraction: Switched to Claude 3 Haiku ($0.00025/1K tokens) ‚Äî 99% accuracy, **99.2% cost reduction**

**New monthly bill: $1,200** (90% reduction)

**The kicker:** Same code works for all models. Switch models by changing one parameter. A/B testing takes minutes, not days.

---

## The Complete Feature Set

### Semantic Model Search

**Find Models by Capability:**
```python
# Search by natural language description
models = search_models(
    bindings={
        "query_vec": {"_embedding_text": "code analysis and reasoning"}
    }
)

# Returns models ranked by semantic similarity
# Understands "code analysis" means: logic, syntax, debugging, refactoring
```

**Custom SQL Queries:**
```python
# Find cheap models with large context
models = search_models(
    sql="""SELECT id, context_length, prompt_cost, completion_cost,
           vec_distance_cosine(embedding, vec_f32(:query_vec)) as similarity 
           FROM models 
           WHERE context_length > 100000 AND prompt_cost < 0.0001
           ORDER BY similarity LIMIT 10""",
    bindings={"query_vec": {"_embedding_text": "document analysis"}}
)
```

**Non-Semantic Queries:**
```python
# Pure SQL filtering (no semantic search)
models = search_models(
    sql="SELECT id, context_length, description FROM models ORDER BY context_length DESC LIMIT 5"
)
```

**Why semantic search matters:** Vector embeddings capture meaning. "code analysis" matches models good at "programming", "debugging", "syntax", "logic" ‚Äî even if those exact words aren't in the description.

### Model Discovery

**List All Models:**
```python
# Get complete model catalog
models = list_available_models()

# Returns: TSV format with all model details
# Columns: id, name, context_length, pricing, modality, provider, etc.
```

**Filter by Criteria:**
```python
# Find specific models
models = list_available_models(
    search_criteria={
        "provider": "anthropic",
        "min_context_length": 100000,
        "max_prompt_price": 0.001,
        "modality": "text->text"
    },
    max_results=10
)
```

**Available Filters:**
- `modality`: "text->text", "text->image", "image->text", etc.
- `min_context_length`: Minimum context window size
- `max_prompt_price`: Maximum price per 1K input tokens
- `max_completion_price`: Maximum price per 1K output tokens
- `provider`: Filter by provider (anthropic, openai, google, etc.)
- `text_match`: Regex pattern for name/description
- `case_sensitive`: Case-sensitive text matching

**JSON Output:**
```python
# Get full JSON instead of TSV
models = list_available_models(
    json=True,
    columns=["id", "context_length", "pricing", "description"]
)
```

**Why list_available_models matters:** Always pulls fresh data from OpenRouter API. Auto-refreshes the local database. Ensures search_models has current data.

### Chat Completions

**Basic Chat:**
```python
# Send message to any model
response = chat_completion(
    model="anthropic/claude-3-opus",
    messages=[
        {"role": "user", "content": "Explain quantum computing"}
    ]
)
```

**Multi-Turn Conversations:**
```python
# Maintain conversation history
response = chat_completion(
    model="openai/gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful coding assistant"},
        {"role": "user", "content": "Write a Python function to sort a list"},
        {"role": "assistant", "content": "Here's a sorting function..."},
        {"role": "user", "content": "Now optimize it for large lists"}
    ]
)
```

**Streaming Responses:**
```python
# Get responses as they're generated
response = chat_completion(
    model="anthropic/claude-3-sonnet",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

# Process chunks as they arrive
for chunk in response:
    print(chunk['content'], end='', flush=True)
```

**Tool Usage:**
```python
# Models can call functions
response = chat_completion(
    model="openai/gpt-4",
    messages=[{"role": "user", "content": "What's the weather in London?"}],
    tools=[
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {"type": "string"}
                    }
                }
            }
        }
    ],
    tool_choice="auto"
)

# Model returns tool call request
# You execute the function and send results back
```

**Source Content Processing:**
```python
# Include URLs or files in context
response = chat_completion(
    model="anthropic/claude-3-opus",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Analyze this documentation"},
                {"type": "url", "url": "https://docs.example.com/api"}
            ]
        }
    ]
)
```

**Why chat_completion matters:** Same interface for 200+ models. Switch models by changing one parameter. No code rewrite, no API changes.

### Account Management

**Check Credits:**
```python
# Get current balance
credits = get_credits()

# Returns: {
#   "balance": 25.50,
#   "usage_this_month": 12.30,
#   "rate_limit": {...}
# }
```

**Get Generation Details:**
```python
# Retrieve specific generation info
generation = get_generation(generation_id="gen_abc123")

# Returns: Full details including cost, tokens, model used
```

**Why account management matters:** Track spending, monitor usage, avoid surprises. Essential for production deployments.

---

## Advanced Use Cases

### Intelligent Model Selection

```python
# Find best model for specific task and budget
def find_optimal_model(task_description, max_price_per_1k, min_context):
    models = search_models(
        sql="""SELECT id, context_length, prompt_cost, completion_cost,
               vec_distance_cosine(embedding, vec_f32(:query_vec)) as similarity 
               FROM models 
               WHERE context_length >= :min_context 
               AND prompt_cost <= :max_price
               ORDER BY similarity LIMIT 10""",
        bindings={
            "query_vec": {"_embedding_text": task_description},
            "min_context": min_context,
            "max_price": max_price_per_1k
        }
    )
    
    # Test top 3 models with sample
    best_model = None
    best_score = 0
    
    for model in models[:3]:
        result = chat_completion(
            model=model['id'],
            messages=[{"role": "user", "content": "Sample task..."}]
        )
        score = evaluate_quality(result)
        if score > best_score:
            best_score = score
            best_model = model
    
    return best_model

# Use it
model = find_optimal_model(
    task_description="code review and bug detection",
    max_price_per_1k=0.001,
    min_context=32000
)
```

### Cost-Optimized Pipeline

```python
# Use different models for different stages
def process_customer_support(ticket):
    # Stage 1: Classification (cheap, fast)
    classifier_models = search_models(
        bindings={"query_vec": {"_embedding_text": "fast text classification"}},
        sql="SELECT id FROM models WHERE prompt_cost < 0.0001 ORDER BY similarity LIMIT 1"
    )
    
    category = chat_completion(
        model=classifier_models[0]['id'],
        messages=[{"role": "user", "content": f"Classify this ticket: {ticket}"}]
    )
    
    # Stage 2: Response generation (quality matters)
    if category == "technical":
        # Use smarter model for technical issues
        response_models = search_models(
            bindings={"query_vec": {"_embedding_text": "technical explanation"}},
            sql="SELECT id FROM models WHERE context_length > 32000 ORDER BY similarity LIMIT 1"
        )
    else:
        # Use cheaper model for simple issues
        response_models = search_models(
            bindings={"query_vec": {"_embedding_text": "customer service response"}},
            sql="SELECT id FROM models WHERE prompt_cost < 0.0005 ORDER BY similarity LIMIT 1"
        )
    
    response = chat_completion(
        model=response_models[0]['id'],
        messages=[{"role": "user", "content": f"Respond to: {ticket}"}]
    )
    
    return response
```

### Multi-Model Consensus

```python
# Get answers from multiple models, combine results
def consensus_answer(question, num_models=3):
    # Find diverse models good at the task
    models = search_models(
        bindings={"query_vec": {"_embedding_text": "reasoning and analysis"}},
        sql="""SELECT id, provider 
               FROM models 
               WHERE context_length > 8000 
               ORDER BY similarity LIMIT 10"""
    )
    
    # Select models from different providers
    selected = []
    providers_used = set()
    for model in models:
        if model['provider'] not in providers_used:
            selected.append(model)
            providers_used.add(model['provider'])
            if len(selected) >= num_models:
                break
    
    # Get answers from each
    answers = []
    for model in selected:
        response = chat_completion(
            model=model['id'],
            messages=[{"role": "user", "content": question}]
        )
        answers.append(response['content'])
    
    # Synthesize consensus
    synthesis = chat_completion(
        model="anthropic/claude-3-opus",  # Use best model for synthesis
        messages=[{
            "role": "user",
            "content": f"Synthesize these {num_models} answers: {answers}"
        }]
    )
    
    return synthesis
```

### Automatic Fallback

```python
# Try models in order until one succeeds
def robust_completion(messages, preferred_models):
    for model_id in preferred_models:
        try:
            response = chat_completion(
                model=model_id,
                messages=messages
            )
            return response
        except Exception as e:
            print(f"Model {model_id} failed: {e}")
            continue
    
    # All preferred models failed, find alternative
    fallback_models = search_models(
        sql="SELECT id FROM models WHERE context_length > 8000 LIMIT 5"
    )
    
    for model in fallback_models:
        try:
            response = chat_completion(
                model=model['id'],
                messages=messages
            )
            return response
        except:
            continue
    
    raise Exception("All models failed")

# Use it
response = robust_completion(
    messages=[{"role": "user", "content": "Analyze this code..."}],
    preferred_models=["openai/gpt-4", "anthropic/claude-3-opus", "google/gemini-pro"]
)
```

---

## Usage Examples

### Semantic Model Search
```json
{
  "input": {
    "operation": "search_models",
    "tool_unlock_token": "YOUR_TOKEN",
    "bindings": {
      "query_vec": {"_embedding_text": "code analysis and reasoning"}
    }
  }
}
```

### Custom SQL Search
```json
{
  "input": {
    "operation": "search_models",
    "tool_unlock_token": "YOUR_TOKEN",
    "sql": "SELECT id, context_length, prompt_cost FROM models WHERE context_length > 100000 ORDER BY prompt_cost LIMIT 5"
  }
}
```

### List Models with Filters
```json
{
  "input": {
    "operation": "list_available_models",
    "tool_unlock_token": "YOUR_TOKEN",
    "search_criteria": {
      "provider": "anthropic",
      "min_context_length": 100000
    },
    "max_results": 10
  }
}
```

### Chat Completion
```json
{
  "input": {
    "operation": "chat_completion",
    "tool_unlock_token": "YOUR_TOKEN",
    "model": "anthropic/claude-3-opus",
    "messages": [
      {"role": "user", "content": "Explain quantum computing"}
    ]
  }
}
```

### Streaming Chat
```json
{
  "input": {
    "operation": "chat_completion",
    "tool_unlock_token": "YOUR_TOKEN",
    "model": "openai/gpt-4",
    "messages": [
      {"role": "user", "content": "Write a long essay"}
    ],
    "stream": true
  }
}
```

### Check Credits
```json
{
  "input": {
    "operation": "get_credits",
    "tool_unlock_token": "YOUR_TOKEN"
  }
}
```

---

## Technical Architecture

### Model Database

**Auto-Refresh System:**
- Pulls fresh data from OpenRouter API
- Updates local SQLite database
- 24-hour cache (configurable)
- Atomic updates (no partial data)

**Vector Embeddings:**
- Each model description embedded with Qwen 0.6B
- 1024-dimensional vectors
- Cosine similarity for semantic search
- Cached for performance

**Database Schema:**
```sql
CREATE TABLE models (
    id TEXT PRIMARY KEY,
    name TEXT,
    description TEXT,
    context_length INTEGER,
    prompt_cost REAL,
    completion_cost REAL,
    provider TEXT,
    modality TEXT,
    architecture TEXT,
    embedding BLOB,  -- F32 vector
    last_updated TIMESTAMP
)
```

### Semantic Search

**How It Works:**
1. User provides search text: "code analysis"
2. Text embedded to 1024-dim vector
3. Vector compared to all model embeddings
4. Cosine similarity calculated
5. Results ranked by similarity
6. Optional SQL filters applied

**Why It's Powerful:**
- Understands synonyms: "code" = "programming" = "software"
- Captures concepts: "analysis" = "review" + "debugging" + "optimization"
- No keyword matching: semantic meaning, not text matching

### API Integration

**Direct HTTP Calls:**
- Uses requests library
- Handles rate limiting
- Automatic retries
- Error propagation

**Streaming Support:**
- Server-Sent Events (SSE)
- Chunked transfer encoding
- Real-time processing
- Memory efficient

**Authentication:**
- API key from config
- Automatic header injection
- Secure storage
- Per-request override

---

## Performance Considerations

### Database Operations
- Model list: ~50-200ms (cached)
- Semantic search: ~100-500ms (vector comparison)
- SQL query: ~10-50ms (indexed)
- Database refresh: ~2-5 seconds (once per 24h)

### API Calls
- Chat completion: 500ms-5s (model-dependent)
- Streaming: First token ~500ms, then continuous
- Model list fetch: ~1-2 seconds
- Credits check: ~100-300ms

### Caching Strategy
- Model database: 24-hour cache
- Embeddings: Permanent (until model changes)
- API responses: Not cached (always fresh)

---

## Limitations & Considerations

### OpenRouter Account Required
- **API Key Needed:** Get from https://openrouter.ai/keys
- **Credits Required:** Pay-as-you-go pricing
- **Rate Limits:** Vary by plan
- **Model Availability:** Some models require approval

### Model Variations
- **Context Lengths:** Vary widely (4K to 1M+ tokens)
- **Pricing:** Ranges from $0.0001 to $0.03+ per 1K tokens
- **Capabilities:** Not all models support tools/streaming
- **Availability:** Some models may be temporarily unavailable

### Semantic Search Accuracy
- **Embedding Quality:** Depends on model description quality
- **Ambiguous Queries:** May return unexpected results
- **Language:** English descriptions only
- **Updates:** New models need re-embedding

### API Limitations
- **Network Required:** All operations need internet
- **Latency:** API calls add network overhead
- **Errors:** Provider outages affect availability
- **Costs:** Every call costs money

---

## Why This Tool is Unmatched

**1. Semantic Discovery**  
Find models by capability, not name. "Good at math" ‚Üí Instant ranked list.

**2. 200+ Models, One API**  
Every major AI in one place. Same code, any model.

**3. Price Optimization**  
Filter by cost, find cheapest model that works. 90%+ cost reduction possible.

**4. Auto-Refresh**  
Model database updates daily. Always current, always accurate.

**5. Vector Search**  
Embeddings capture meaning. Understands synonyms, concepts, relationships.

**6. Custom SQL**  
Full database access. Complex queries, precise filtering.

**7. Streaming Support**  
Real-time responses. Memory efficient, user-friendly.

**8. Tool Integration**  
Models can call functions. Build AI agents, not just chatbots.

**9. Multi-Provider**  
OpenAI, Anthropic, Google, Meta, Mistral ‚Äî all in one place.

**10. Production-Ready**  
Error handling, rate limiting, retries. Battle-tested.

---

## Powered by MCP-Link

This tool is part of the [MCP-Link Server](https://github.com/AuraFriday/mcp-link-server) ‚Äî the only MCP server with OpenRouter integration and semantic model search built-in.

### What's Included

**Isolated Python Environment:**
- SQLite with vector extensions
- Qwen 0.6B embeddings bundled
- Requests library included
- Zero configuration required

**Model Database:**
- Auto-refreshing catalog
- Vector embeddings pre-computed
- Optimized for fast search
- 24-hour cache

**API Integration:**
- Automatic authentication
- Error handling
- Rate limit management
- Streaming support

### Get MCP-Link

Download the installer for your platform:
- [Windows](https://github.com/AuraFriday/mcp-link-server/releases/latest)
- [Mac (Apple Silicon)](https://github.com/AuraFriday/mcp-link-server/releases/latest)
- [Mac (Intel)](https://github.com/AuraFriday/mcp-link-server/releases/latest)
- [Linux](https://github.com/AuraFriday/mcp-link-server/releases/latest)

**Installation is automatic. Dependencies are included. It just works.**

---

## Technical Specifications

**Supported Models:** 200+ (OpenAI, Anthropic, Google, Meta, Mistral, etc.)  
**Search Method:** Vector embeddings (Qwen 0.6B, 1024-dim, cosine similarity)  
**Database:** SQLite with vec0 extension  
**Cache Duration:** 24 hours (configurable)  
**API:** OpenRouter REST API  
**Streaming:** Server-Sent Events (SSE)  
**Authentication:** API key (from config or parameter)  

**Performance:**
- Semantic search: 100-500ms
- Model listing: 50-200ms
- Chat completion: 500ms-5s
- Database refresh: 2-5s (once per 24h)

---

## License & Copyright

Copyright ¬© 2025 Christopher Nathan Drake

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at:

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

AI Training Permission: You are permitted to use this software and any
associated content for the training, evaluation, fine-tuning, or improvement
of artificial intelligence systems, including commercial models.

SPDX-License-Identifier: Apache-2.0

Part of the Aura Friday MCP-Link Server project.

---

## Support & Community

**Issues & Feature Requests:**  
[GitHub Issues](https://github.com/AuraFriday/mcp-link/issues)

**Documentation:**  
[MCP-Link Documentation](https://aurafriday.com/)

**OpenRouter:**  
[Get API Key](https://openrouter.ai/keys) | [Model Catalog](https://openrouter.ai/models)

**Community:**  
Join other developers building multi-model AI applications.

